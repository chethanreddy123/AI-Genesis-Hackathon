{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5461bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import google.generativeai as palm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fb54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key='AIzaSyA1fu-ob27CzsJozdr6pHd96t5ziaD87wM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b744778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/text-bison-001\n"
     ]
    }
   ],
   "source": [
    "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "model = models[0].name\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0a8d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import langchain\n",
      "import googlemaps\n",
      "\n",
      "# Create a Langchain client\n",
      "client = langchain.Client()\n",
      "\n",
      "# Get the Palm 2 API key from Google\n",
      "api_key = \"YOUR_API_KEY\"\n",
      "\n",
      "# Create a Google Maps client\n",
      "gmaps_client = googlemaps.Client(api_key)\n",
      "\n",
      "# Get the location of the Palm 2\n",
      "palm_2_location = gmaps_client.geocode(\"Palm 2\")[0][\"geometry\"][\"location\"]\n",
      "\n",
      "# Create a Langchain request\n",
      "request = langchain.Request(\n",
      "    text=\"What is the weather like at the Palm 2?\",\n",
      "    location=palm_2_location,\n",
      ")\n",
      "\n",
      "# Get the Langchain response\n",
      "response = client.request(request)\n",
      "\n",
      "# Print the Langchain response\n",
      "print(response)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = \"write python code to for using Langchain with palm 2 api of google\"\n",
    "\n",
    "completion = palm.generate_text(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    temperature=0,\n",
    "    # The maximum length of the response\n",
    "    max_output_tokens=800,\n",
    ")\n",
    "\n",
    "print(completion.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0910d",
   "metadata": {},
   "source": [
    "## Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8d6964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import langchain\n",
      "import googlemaps\n",
      "\n",
      "# Create a Langchain client\n",
      "client = langchain.Client()\n",
      "\n",
      "# Get the Palm 2 API key from Google\n",
      "api_key = \"YOUR_API_KEY\"\n",
      "\n",
      "# Create a Google Maps client\n",
      "gmaps_client = googlemaps.Client(api_key)\n",
      "\n",
      "# Get the location of the Palm 2\n",
      "palm_2_location = gmaps_client.geocode(\"Palm 2\")[0][\"geometry\"][\"location\"]\n",
      "\n",
      "# Create a Langchain request\n",
      "request = langchain.Request(\n",
      "    text=\"What is the weather like at the Palm 2?\",\n",
      "    location=palm_2_location,\n",
      ")\n",
      "\n",
      "# Get the Langchain response\n",
      "response = client.request(request)\n",
      "\n",
      "# Print the Langchain response\n",
      "print(response)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as palm\n",
    "\n",
    "palm.configure(api_key='AIzaSyA1fu-ob27CzsJozdr6pHd96t5ziaD87wM')\n",
    "\n",
    "def generate_text_with_palm(prompt):\n",
    "    models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
    "    model = models[0].name\n",
    "\n",
    "    completion = palm.generate_text(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_output_tokens=800,\n",
    "    )\n",
    "\n",
    "    return completion.result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prompt = \"write python code to for using Langchain with palm 2 api of google\"\n",
    "    generated_text = generate_text_with_palm(prompt)\n",
    "    print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af70aee",
   "metadata": {},
   "source": [
    "## Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23857a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "288ec6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'langchain.llms' from '/Users/achethanreddy/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/llms/__init__.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d11f768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anthropic',\n",
       " 'ArxivAPIWrapper',\n",
       " 'Banana',\n",
       " 'BaseCache',\n",
       " 'BasePromptTemplate',\n",
       " 'CerebriumAI',\n",
       " 'Cohere',\n",
       " 'ConversationChain',\n",
       " 'ElasticVectorSearch',\n",
       " 'FAISS',\n",
       " 'FewShotPromptTemplate',\n",
       " 'ForefrontAI',\n",
       " 'GoogleSearchAPIWrapper',\n",
       " 'GoogleSerperAPIWrapper',\n",
       " 'GooseAI',\n",
       " 'HuggingFaceHub',\n",
       " 'HuggingFacePipeline',\n",
       " 'HuggingFaceTextGenInference',\n",
       " 'InMemoryDocstore',\n",
       " 'LLMBashChain',\n",
       " 'LLMChain',\n",
       " 'LLMCheckerChain',\n",
       " 'LLMMathChain',\n",
       " 'LlamaCpp',\n",
       " 'MRKLChain',\n",
       " 'Modal',\n",
       " 'OpenAI',\n",
       " 'Optional',\n",
       " 'PALChain',\n",
       " 'Petals',\n",
       " 'PipelineAI',\n",
       " 'PowerBIDataset',\n",
       " 'Prompt',\n",
       " 'PromptTemplate',\n",
       " 'QAWithSourcesChain',\n",
       " 'ReActChain',\n",
       " 'SQLDatabase',\n",
       " 'SQLDatabaseChain',\n",
       " 'SagemakerEndpoint',\n",
       " 'SearxSearchWrapper',\n",
       " 'SelfAskWithSearchChain',\n",
       " 'SerpAPIChain',\n",
       " 'SerpAPIWrapper',\n",
       " 'StochasticAI',\n",
       " 'VectorDBQA',\n",
       " 'VectorDBQAWithSourcesChain',\n",
       " 'Wikipedia',\n",
       " 'WikipediaAPIWrapper',\n",
       " 'WolframAlphaAPIWrapper',\n",
       " 'Writer',\n",
       " '__all__',\n",
       " '__annotations__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'agents',\n",
       " 'base_language',\n",
       " 'cache',\n",
       " 'callbacks',\n",
       " 'chains',\n",
       " 'chat_models',\n",
       " 'debug',\n",
       " 'docstore',\n",
       " 'document_loaders',\n",
       " 'embeddings',\n",
       " 'env',\n",
       " 'formatting',\n",
       " 'graphs',\n",
       " 'input',\n",
       " 'llm_cache',\n",
       " 'llms',\n",
       " 'load',\n",
       " 'math_utils',\n",
       " 'memory',\n",
       " 'output_parsers',\n",
       " 'prompts',\n",
       " 'requests',\n",
       " 'schema',\n",
       " 'sql_database',\n",
       " 'text_splitter',\n",
       " 'tools',\n",
       " 'utilities',\n",
       " 'utils',\n",
       " 'vectorstores',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(langchain.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc228b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
